# Calculate the theoretical CDF assuming a standard normal distribution
theoreticalCDF <- pnorm(data)
# Calculate the test statistic D
D <- max(abs(empiricalCDF - theoreticalCDF))
# Initialize p-value
p_value <- 0
# Loop to calculate the sum in the formula
for (k in 1:1000) {  # Adjust the number of iterations as needed
p_value <- sqrt(2 * pi) / D * exp(-(((2 * k) - 1)^2) * pi^2 / (8 * D^2))
}
return(p_value)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
p_value <- calculate_p_value(data)
p_value
calculate_p_value <- function(data) {
# Calculate the ECDF of the data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Calculate the theoretical CDF assuming a standard normal distribution
theoreticalCDF <- pnorm(data)
# Calculate the test statistic D
D <- max(abs(empiricalCDF - theoreticalCDF))
# Initialize p-value
p_value <- 0
# Loop to calculate the sum in the formula
for (k in 1:1000) {  # Adjust the number of iterations as needed
p_value <- p_value + sqrt(2 * pi) / D * exp(-(((2 * k) - 1)^2) * pi^2 / (8 * D^2))
}
return(p_value)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
p_value <- calculate_p_value(data)
p_value
sprintf(5.652523e-29)
sprintf(5.652523e-29)
sprintf("%.20f",5.652523e-29)
sprintf("%.20f",2.22e-16)
View(ks_result)
View(ks_result)
View(ks_result)
set.seed(123)
data <- data.frame(x = runif(200,1,10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
View(data)
model <- lm(y ~ x, data = data)
summary(model)
plot(data$x, data$y)
abline(model, col = "red")
glm(data$x ~ data$y, data = data,
family = "BFGS")
View(data)
View(model)
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = objective_function, x = x, y = y, method = "BFGS")
objective_function <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = objective_function, x = x, y = y, method = "BFGS")
set.seed(123)
data <- data.frame(x = runif(200,1,10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Using LM
model <- lm(y ~ x, data = data)
summary(model)
plot(data$x, data$y)
abline(model, col = "red")
# Using BFGS
glm(data$x ~ data$y, data = data,
family = "BFGS")
objective_function <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = objective_function, x = x, y = y, method = "L-BFGS-B")
set.seed(123)
data <- data.frame(x = runif(200,1,10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Using LM
model <- lm(y ~ x, data = data)
summary(model)
plot(data$x, data$y)
abline(model, col = "red")
# Using BFGS
glm(data$x ~ data$y, data = data,
family = "BFGS")
set.seed(123)
data <- data.frame(x = runif(200,1,10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Using LM
model <- lm(y ~ x, data = data)
summary(model)
plot(data$x, data$y)
abline(model, col = "red")
# Using BFGS
objective_function <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = objective_function, x = x, y = y, method = "BFGS")
set.seed(123)
data <- data.frame(x = runif(200,1,10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Using LM
model <- lm(y ~ x, data = data)
summary(model)
plot(data$x, data$y)
abline(model, col = "red")
# Using BFGS
objective_function <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = objective_function, x = data$x, y = data$y, method = "BFGS")
# Extract the coefficients
coefficients <- result$par
print(coefficients)
o_function <- function(beta, x, y) {
y_hat <- beta[0] + beta[1] * x
sum((y - y_hat)^2)
}
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = o_function, x = data$x, y = data$y, method = "BFGS")
# Extract the coefficients
coefficients <- result$par
print(coefficients)
o_function <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = o_function, x = data$x, y = data$y, method = "BFGS")
# Extract the coefficients
coefficients <- result$par
print(coefficients)
calculate_p_value <- function(data) {
# Calculate the ECDF of the data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Calculate the theoretical CDF assuming a standard normal distribution
theoreticalCDF <- pnorm(data)
# Calculate the test statistic D
D <- max(abs(theoreticalCDF - empiricalCDF))
# Initialize p-value
p_value <- 0
# Loop to calculate the sum in the formula
for (k in 1:1000) {  # Adjust the number of iterations as needed
p_value <- p_value + sqrt(2 * pi) / D * exp(-(((2 * k) - 1)^2) * pi^2 / (8 * D^2))
}
return(p_value)
}
p_value
set.seed(123)
data <- rcauchy(1000)
p_value <- calculate_p_value(data)
p_value
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data, "pnorm")
# Print the result
print(ks_result)
p_value
D
D <- max(abs(empiricalCDF - theoreticalCDF))
calculate_p_value <- function(data) {
# Calculate the ECDF of the data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Calculate the theoretical CDF assuming a standard normal distribution
theoreticalCDF <- pnorm(data)
# Calculate the test statistic D
D <- max(abs(empiricalCDF - theoreticalCDF))
# Initialize p-value
p_value <- 0
# Loop to calculate the sum in the formula
for (k in 1:1000) {  # Adjust the number of iterations as needed
p_value <- p_value + sqrt(2 * pi) / D * exp(-(((2 * k) - 1)^2) * pi^2 / (8 * D^2))
}
return(p_value)
}
set.seed(123)
data <- rcauchy(1000)
p_value <- calculate_p_value(data)
p_value
D
D <- max(abs(empiricalCDF - theoreticalCDF))
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data, "pnorm")
# Print the result
print(ks_result)
sprintf("%.20f",5.652523e-29)
sprintf("%.20f",2.22e-16)
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Calculate the theoretical CDF assuming a standard normal distribution
theoreticalCDF <- pnorm(data)
# Calculate the test statistic D
D <- max(abs(empiricalCDF - theoreticalCDF))
calculate_p_value <- function(data) {
# Create empirical distribution
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Calculate the test statistic D
D <- max(abs(empiricalCDF - pnorm(data))
# Initialize p-value
p_value <- 0
calculate_p_value <- function(data) {
# Create empirical distribution
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Calculate the test statistic D
D <- max(abs(empiricalCDF - pnorm(data))
# Initialize p-value
p_value <- 0
calculate_p_value <- function(data) {
# Create empirical distribution
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Calculate the theoretical CDF assuming a standard normal distribution
theoreticalCDF <- pnorm(data)
# Calculate the test statistic D
D <- max(abs(empiricalCDF - theoreticalCDF))
# Initialize p-value
p_value <- 0
# Loop to calculate the sum in the formula
for (k in 1:1000) {
p_value <- p_value + sqrt(2 * pi) / D * exp(-(((2 * k) - 1)^2) * pi^2 / (8 * D^2))
}
return(p_value)
}
set.seed(123)
data <- rcauchy(1000)
p_value <- calculate_p_value(data)
p_value
D
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data, "pnorm")
# Print the result
print(ks_result)
sprintf("%.20f",5.652523e-29)
sprintf("%.20f",2.22e-16)
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data, "pnorm(data)")
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data, "pcauchy")
# Print the result
print(ks_result)
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data, "pnorm")
# Print the result
print(ks_result)
Perform Kolmogorov-Smirnov test
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data, "pnorm")
# Print the result
print(ks_result)
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data)
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(data, "pnorm")
# Print the result
print(ks_result)
nr_function <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = nr_function, x = data$x, y = data$y, method = "BFGS")
set.seed(123)
data <- data.frame(x = runif(200,1,10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Using LM
model <- lm(y ~ x, data = data)
summary(model)
plot(data$x, data$y)
abline(model, col = "red")
# Using BFGS
nr_function <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
initial_guess <- c(0, 1) # Initial guess for intercept and slope
result <- optim(par = initial_guess, fn = nr_function, x = data$x, y = data$y, method = "BFGS")
# Extract the coefficients
coefficients <- result$par
print(coefficients)
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(empiricalCDF, theoreticalCDF)
calculate_p_value <- function(data) {
# Create empirical distribution
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Calculate the theoretical CDF assuming a standard normal distribution
theoreticalCDF <- pnorm(data)
# Calculate the test statistic D
D <- max(abs(empiricalCDF - theoreticalCDF))
# Initialize p-value
p_value <- 0
# Loop to calculate the sum in the formula
for (k in 1:1000) {
p_value <- p_value + sqrt(2 * pi) / D * exp(-(((2 * k) - 1)^2) * pi^2 / (8 * D^2))
}
return(p_value)
}
set.seed(123)
data <- rcauchy(1000)
p_value <- calculate_p_value(data)
p_value
D
# Perform Kolmogorov-Smirnov test
ks_result <- ks.test(empiricalCDF, theoreticalCDF)
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("C:/Users/carty/Downloads/yelp_data_small.csv",
stringsAsFactors=FALSE,
encoding = "utf-8")
View(data)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
corpus <- corpus(data,
text_field = "text")
prop.table(table(corpus$sentiment))
# create tokens
toks <- tokens(corpus,
include_docvars = TRUE)
toks <- toks %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"), padding = TRUE) %>%
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE)
# find collocations
cols <- textstat_collocations(toks,
method = "lambda",
size = 2,
min_count = 5,
smoothing = 0.5)
cols <- cols[order(-cols$count),] # sort detected collocations by count (descending)
# replace collocations
toks <- tokens_compound(toks, pattern = cols[cols$z > 10,])
# remove whitespace placeholders
toks <- tokens_remove(tokens(toks), "")
# remove other uninformative non-text
toks <- tokens(toks,
#remove_numbers = TRUE, # we keep numbers because of star discussion
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
# create DFM
dfm <- dfm(toks) # create DFM
# trim DFM
dfm <- dfm_trim(dfm, min_termfreq = 30)
# weight DFM
dfm <- dfm_tfidf(dfm) # weight DFM
dfm <- # your final object should be called dfm
View(dfm)
library(caret)
install.package(caret)
install.package(caret)
install.packages("caret")
library(caret)
tmpdata <- convert(dfm, to = "data.frame", docvars = NULL)
tmpdata <- tmpdata[, -1]
sentiment <- as.factor(dfm@docvars$sentiment)
ldata <- as.data.frame(cbind(sentiment, tmpdata))
train_row_nums <- createDataPartition(y = ldata$sentiment, # set sentiment as the Y variable in caret
p = 0.8, # fill in the blank
list=FALSE)
Train <- ldata[train_row_nums, ]
Test <- ldata[-train_row_nums, ]
library('doParallel') # for parallel processing
install.packages('doParallel')
install.packages('naivebayes')
install.packages('naivebayes')
library('doParallel') # for parallel processing
library('naivebayes') # naive bayes classifier
library('naivebayes') # model performance
# 1. Set grid search for NB classifier
tgrid <- expand.grid(laplace = c(0,0.5,1.0),
usekernel = c(TRUE, FALSE),
adjust=c(0.75, 1, 1.25, 1.5))
# 2. Set up 5-fold cross-validation, repeated 3 times
train_control <- trainControl(method = "repeatedcv",
number = 5,
repeats = 3,
classProbs= TRUE,
summaryFunction = multiClassSummary,
selectionFunction = "best", # select the model with the best performance metric
verboseIter = TRUE)
# 3. Set parallel processing cluster
cl <- makePSOCKcluster(5) # create number of copies of R to run in parallel and communicate over sockets
registerDoParallel(cl) # register parallel backed with foreach package
# 4. Train model
nb_train <- train(sentiment ~ .,
data = Train,
method = "naive_bayes",
metric = "F1",
trControl = train_control,
tuneGrid = tgrid,
allowParallel= TRUE
)
install.packages('MLmetrics')
library('doParallel') # for parallel processing
library('naivebayes') # naive bayes classifier
library('naivebayes') # model performance
# 1. Set grid search for NB classifier
tgrid <- expand.grid(laplace = c(0,0.5,1.0),
usekernel = c(TRUE, FALSE),
adjust=c(0.75, 1, 1.25, 1.5))
# 2. Set up 5-fold cross-validation, repeated 3 times
train_control <- trainControl(method = "repeatedcv",
number = 5,
repeats = 3,
classProbs= TRUE,
summaryFunction = multiClassSummary,
selectionFunction = "best", # select the model with the best performance metric
verboseIter = TRUE)
# 3. Set parallel processing cluster
cl <- makePSOCKcluster(5) # create number of copies of R to run in parallel and communicate over sockets
registerDoParallel(cl) # register parallel backed with foreach package
# 4. Train model
nb_train <- train(sentiment ~ .,
data = Train,
method = "naive_bayes",
metric = "F1",
trControl = train_control,
tuneGrid = tgrid,
allowParallel= TRUE
)
stopCluster(cl) # stop parallel process once job is done
# save results
saveRDS(nb_train, file = "nb_train.RDS")
View(nb_train)
confusionMatrix(reference = Test$sentiment, data = pred, mode='everything', positive='neg')
pred <- predict(nb_train, newdata = Test)
confusionMatrix(reference = Test$sentiment, data = pred, mode='everything', positive='neg')
var_imp <- varImp(object = nb_train) # calculate feature importance
plot(var_imp, top = 20) # plot top 20 most important features
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("stringr", "tidyverse"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
# PART 1
# Fit an additive model. Provide the summary output, the global null hypothesis,
# and p-value. Please describe the results and provide a conclusion.
# load data
load(url("https://github.com/ASDS-TCD/StatsII_Spring2024/blob/main/datasets/climateSupport.RData?raw=true"))
data <- climateSupport
head(data)
summary(data)
str(data)
data$choice <- as.factor(ifelse(data$choice == "Supported", 1, 0))
data$choice
data$countries <- factor(data$countries, ordered = FALSE, levels = c("20 of 192", "80 of 192", "160 of 192"), labels = c("_20_192", "_80_192", "_160_192"))
data$sanctions <- factor(data$sanctions, ordered = FALSE, levels = c("None", "5%", "15%", "20%"), labels = c("none", "_5_percent", "_15_percent", "_20_percent"))
data$countries <- as.factor(data$countries)
data$sanctions <- as.factor(data$sanctions)
logit_base <- glm(choice ~ sanctions + countries, data = data, family = binomial(link = "logit"))
summary(logit_base)
nullMod <- glm(choice ~ 1, # 1 = fit an intercept only (i.e. sort of a "mean")
data = data,
family = "binomial")
summary(nullMod)
#  Run an anova test on the model compared to the null model
anova(nullMod, logit_base, test = "Chisq")
# Add an interactive term
Model_1 <- glm(choice ~ sanctions + countries, data = data, family = binomial(link = "logit"))
Model_Interactive <- glm(choice ~ sanctions * countries, data = data, family = binomial(link = "logit"))
#H0: Bj = 0
#H0: B1 = B2 = B3 = 0
# can use anova
# interactive = slides week 4
#HA: at least one slope = 0
#Z test
#If P value low enough can conclude that have enough evidence to reject null hypothesis that estimated coefficient does not equal zero
summary(Model_Interactive)
summary(Model_Interactive)
anova(logit_base, Model_Interactive, test = "LRT")
exp(1.065)/(1+exp(1.065))
exp(0.0637)/(1+exp(0.0637))
